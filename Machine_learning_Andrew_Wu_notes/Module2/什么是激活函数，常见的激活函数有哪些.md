# 激活函数

## 作用
1. **引入非线性**  
   - 学习复杂的模式（如图像识别）。  
   - 如果神经网络中每一层一直进行线性变换，多个堆叠起来依旧是线性模型。  

2. **提供分段/压缩效果**  
   - **Sigmoid**：压缩到 (0, 1)，可以用来表示概率问题。  
   - **Tanh**：压缩到 (-1, 1)，具有对称性。  
   - **ReLU**：只保留正数信息，保证稀疏性，计算高效。  

3. **影响梯度传播**  
   - 激活函数决定了误差在反向传播时的梯度大小：  
     - Sigmoid/Tanh → 容易导致梯度消失  
     - ReLU → 缓解梯度消失，但可能导致神经元死亡  

4. **决定输出范围和语义**  
   - 分类问题：Softmax 把输出转成概率分布。  
   - 二分类：Sigmoid 把输出变成 0–1 概率。  
   - 回归问题：有时不加激活（保持线性输出）。  

---

## 常见的激活函数

### 1. Sigmoid
公式：  

$f(x) = \frac{1}{1+e^{-x}}$


- **Pro**  
  - 连续  
  - 关于 x=y 对称，能够处理正负输入值  

- **Con**  
  - 梯度消失：当输入值的绝对值较大时，输出趋近于 0  
  - 计算开销大：涉及指数运算，相比 ReLU 等计算量更大  

---

### 2. ReLU
公式：  

$f(x) = \max(0, x)$


- **Pro**  
  - 计算高效，缓解梯度消失  

- **Con**  
  - 没有上限：\(\lim f \to \infty\)，可能导致梯度爆炸  
  - 非零均值 → 需要归一化处理  
  - 部分神经元不会被激活 → 神经元死亡  

- **改进方法**  
  - **Leaky ReLU**:  
    
    $f(y) = 
    \begin{cases}
    y, & y \geq 0 \\ 
    0.001y, & y < 0
    \end{cases}
   $
  - **Parametric ReLU**:  
    
   $ f(y) = 
    \begin{cases}
    y, & y \geq 0 \\ 
    ay, & y < 0,\quad a \text{为可学习参数}
    \end{cases}
     $

---

### 3. Swish
公式：  
$
f(x) = x \cdot \sigma(x), \quad \sigma(x)=\frac{1}{1+e^{-x}}
$

- **Pro**  
  - 结合了 Sigmoid 的平滑性和 ReLU 的稀疏性  
  - 常常比 ReLU 收敛更快、精度更高  

---

### 4. GLU
- **门控线性单元 (Gated Linear Unit)**  
- 通过门控机制控制信息流动  
![formula for GLU]()

---

### 5. SwiGLU
- **Swish + GLU 的组合**
![formula for SwiGLU](images/SwiGLU.jpg)

- **Pro**  
  - **非线性能力**：通过 Swish 引入非线性，可学习复杂数据模式  
  - **门控特性**：GLU 的门控机制动态调节信息流，更好捕捉长距离依赖  
  - **梯度稳定性**：在负输入区域提供非零梯度，缓解梯度消失  
  - **可学习参数**：参数可训练，增强灵活性与适应性  
  - **计算效率**：性能优良且高效，适合大规模语言模型的训练与推理  


参考资料：
https://github.com/luhengshiwo/LLMForEverybody/blob/main/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%83%BD%E5%9C%A8%E4%BD%BF%E7%94%A8SwiGLU%E4%BD%9C%E4%B8%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F.md