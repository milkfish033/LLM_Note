# 多模态大模型应用开发 — 模拟技术面试问答笔记

---

## 一、模型与算法方向

### **问题1：请解释一下 Transformer 的核心机制以及为什么它在语言和视觉任务中都表现出色？**
**答案：**  
Transformer 的核心在于自注意力机制（Self-Attention），通过计算序列中各位置之间的相关性，模型可以并行捕捉长距离依赖。相比 RNN，Transformer 不依赖时间步展开，因此训练效率更高。  
在语言任务中，它能理解上下文语义；在视觉任务中，ViT 将图片划分为 patch 并作为“词”输入，从而让注意力机制捕捉图像的全局结构，因此在两个模态中都能表现优异。

---

### **问题2：在你使用 LoRA + SFT 微调 Qwen-Plus 的项目中，你是如何选择超参数的？**
**答案：**  
我基于实验验证选择 rank=8、alpha=16、学习率为 2e-4。rank 决定可训练参数规模，太高会过拟合，太低又难以学习特征。  
我通过验证集监控 loss 与 BLEU 指标变化，并在 Prompt 设计上引入模板化结构，最终在保证推理速度的前提下提升了约 38% 的准确率。

---

### **问题3：如果模型在微调后出现“幻觉”问题，你会怎么处理？**
**答案：**  
我会分三步排查：  
1. **数据层面**：检查训练数据是否存在噪声或标注不一致。  
2. **Prompt层面**：加入明确的指令与上下文约束，如“基于事实回答”。  
3. **模型层面**：结合 RAG 检索机制，让模型基于知识库生成，从而减少虚构信息。

---

### **问题4：请详细说明你在英语口语评测平台中如何实现 RAG 流程？**
**答案：**  
我使用向量数据库 Milvus 存储口语评分知识文档。流程包括：  
1. 使用 BGE-small-zh 模型生成嵌入向量；  
2. 对学生发音文本检索相关评分标准段落；  
3. 将检索结果拼接到 LLM Prompt 中，让模型基于规则生成反馈。  
这种结构确保评语具备一致性和可解释性。

---

### **问题5：如果向量数据库召回率不高，你如何优化？**
**答案：**  
我会从以下角度优化：  
- 替换更高质量的嵌入模型（如 bge-large）；  
- 使用多轮 Query Expansion；  
- 调整向量维度与相似度阈值；  
- 利用 reranker（如 cross-encoder）对 top-k 结果进行二次筛选。

---

### **问题6：比较直接微调与 RAG 的区别。**
**答案：**  
微调适用于长期稳定任务（如问答生成），但更新成本高；  
RAG 灵活，可快速注入新知识，适用于动态内容。  
在实际项目中，我会先用 RAG 解决快速上线，再用微调模型优化性能。

---

### **问题7：在 HackHarvard 项目中，你使用 YOLOv8，有哪些结构改进？**
**答案：**  
YOLOv8 引入了 C2f 模块替代 CSPNet，使特征复用更高效；同时支持 anchor-free 预测，提高小目标检测能力。  
我选择 YOLOv8 主要是因为其推理速度快、易于导出 ONNX 部署。

---

### **问题8：如果要结合视觉与语言模型实现异常解释系统，你会如何设计？**
**答案：**  
我会采用两阶段架构：  
1. **视觉部分**：YOLOv8 检测出异常事件。  
2. **语言部分**：将检测结果转化为文本输入给 LLM，通过模板生成自然语言说明。  
必要时可引入 CLIP 进行跨模态对齐，确保文本描述与图像一致。

---

### **问题9：ControlNet 的原理是什么？**
**答案：**  
ControlNet 在扩散模型中冻结原始网络参数，通过附加可学习分支（control branch）注入控制条件，如边缘图、姿态图等。  
这种方式让生成结果更可控而不破坏原模型分布。

---

### **问题10：请描述一次完整的模型训练到部署流程。**
**答案：**  
1. 数据准备 → 清洗 + 划分训练集；  
2. 模型训练 → 使用 LoRA 进行微调；  
3. 模型导出 → 转换为 `.pt` / `.onnx`；  
4. 部署 → 使用 Flask 封装推理接口，Docker 打包；  
5. 运维 → 部署在 AWS EC2，利用 CloudWatch 监控性能与日志。

---

## 二、工程实现与系统设计

### **问题11：请举例说明你设计的一个后端 API 接口。**
**答案：**
```python
@app.route('/api/generate', methods=['POST'])
def generate():
    data = request.json
    prompt = data.get('prompt')
    result = model.generate(prompt)
    return jsonify({"output": result})
