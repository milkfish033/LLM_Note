
[中文](./lora微调的参数怎么选择，lora和Qlora的区别.md) |
[English](./LoRA&QLoRAHyperparameterConfigurationandPracticalGuide.md)

# LoRA 与 QLoRA 超参数配置与实践总结

## 一、LoRA 与 QLoRA 概述
LoRA（Low-Rank Adaptation）和 QLoRA 是参数高效微调（PEFT）技术，旨在在保持模型性能的同时减少训练所需的计算资源。

- **LoRA**：通过在 Transformer 中插入低秩分解的适配器（A、B 矩阵），只训练这部分参数，大幅减少训练成本。
- **QLoRA**：在 LoRA 基础上引入 **4-bit 量化** 技术，将冻结的预训练权重压缩存储并在计算时动态反量化，从而进一步降低显存占用，使得在消费级 GPU 上也能微调大模型。

| 特性        | LoRA                          | QLoRA                               |
|-------------|--------------------------------|--------------------------------------|
| 核心技术    | 低秩适配器                      | 在 LoRA 上加入 4-bit 量化           |
| 显存占用    | 低于全量微调，但高于 QLoRA       | 最低，可在单 GPU 上训练更大模型      |
| 训练速度    | 更快，无额外量化开销             | 稍慢，因动态反量化                   |
| 精度        | 通常略高于 QLoRA                 | 可能有轻微精度损失，但大多可忽略     |
| 适用场景    | 有一定计算资源且注重性能         | 资源受限、低显存或超大模型场景       |

---

## 二、LoRA 微调方法
1. 冻结预训练模型权重。
2. 在 Transformer 的权重矩阵旁添加 **A、B 两个小矩阵**，用于计算增量更新 ΔW。
3. 仅训练 A、B 的参数。
4. 推理时将 LoRA 参数合并入原始权重 W₀ + BA，实现与全量微调相同的推理速度。

---

## 三、QLoRA 微调方法
1. 将模型从 16-bit 浮点数量化为 **4-bit NF4**。
2. 冻结量化后权重。
3. 插入 LoRA 适配器并进行混合精度训练：
   - 训练时将量化权重动态反量化至 16-bit 计算。
   - 仅更新 LoRA 适配器。
4. **双量化（Double Quantization）** 进一步减少显存使用。

---

## 四、关键超参数

### 1. 秩值（r）
- **作用**：控制低秩矩阵的维度，即模型适应任务的“带宽”。
- 典型范围：
  - 简单任务：4–8
  - 中等任务：16–32
  - 复杂推理：64–128
  - 语言迁移：128–256
- 调优策略：从小到大试验，找到性能提升的“拐点”。

### 2. 缩放因子（lora_alpha）
- **作用**：控制适配器对原模型的影响。
- 常用设置：`alpha = 2 × r`（标准）
- 调优建议：
  - 如果训练不稳定 → 减小 alpha
  - 如果模型学习不足 → 增大 alpha

### 3. 目标模块（target_modules）
- **作用**：决定哪些层应用 LoRA 适配器。
- 常用选择：
  - 最小化：`["q_proj", "v_proj"]`
  - 平衡：`["q_proj", "k_proj", "v_proj", "o_proj"]`
  - 最大化（含 FFN 层）：再加入 `["gate_proj", "up_proj", "down_proj"]`

### 4. Dropout（lora_dropout）
- **作用**：防止过拟合。
- 范围：0.0–0.2（常用 0.05–0.1）
- 数据量少 → 较高 Dropout  
  数据量大 → 较低 Dropout

### 5. 偏置（bias）
- 选项：
  - `"none"`：不训练偏置（最常用）
  - `"lora_only"`：仅训练 LoRA 偏置
  - `"all"`：训练所有偏置（增加参数）

### 6. 学习率（learning_rate）
- LoRA 常用：1e-4 ~ 5e-4  
- QLoRA 常用：5e-5 ~ 2e-4（略低）

---

## 五、调优策略
1. **快速扫描**：先用 [8,16,32,64] 等秩值进行短期训练。
2. **分析结果**：关注性能曲线平缓点、训练稳定性与过拟合迹象。
3. **精细微调**：围绕最佳秩值进行细粒度实验。
4. **完整训练**：对前 2 个最佳秩值进行长时间验证。


---

## 七、常见问题与解决方案
1. **训练不稳定**：降低学习率或 alpha，使用梯度裁剪。
2. **过拟合** ：提高 Dropout，降低秩值，或早停。
3. **欠拟合** ：增加秩值或目标模块。
4. **显存不足**：使用 QLoRA 或减少批次大小



## 参考资料
1. 一文说明白LoRA/QLoRA超参数：参数最优选择策略，及正确配置完整指南打造最优微调实践 - Tableau的文章 - 知乎
https://zhuanlan.zhihu.com/p/1938278622218650334

2. LoRA微调参数详解 - 只是没有如果的文章 - 知乎
https://zhuanlan.zhihu.com/p/1953953705192821962