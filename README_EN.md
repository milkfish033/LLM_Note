# LLM Learning Notes

This repository is for recording notes during the learning process of **Large Language Models (LLMs)**.  

[ğŸ‡¨ğŸ‡³ ä¸­æ–‡](./README.md) | [ğŸ‡ºğŸ‡¸ English](./README_EN.md)

Email: kerwinish@gmail.com

ğŸš© Latest Update: 10/24/2025

## Table of Contents
```
LLM_NOTE/
â”œâ”€â”€ DecisionTree/
â”‚   â”œâ”€â”€ DecisionTreeCn.md
â”‚   â””â”€â”€ DecisionTreeEn.md
â”‚
â”œâ”€â”€ Essay_reading/
â”‚   â”œâ”€â”€ Bert_note_cn.md
â”‚   â”œâ”€â”€ Bert_note_en.md
â”‚   â””â”€â”€ Bert.pdf
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ GLU.jpg
â”‚   â””â”€â”€ SwiGLU.jpg
â”‚
â”œâ”€â”€ lora/
â”‚   â”œâ”€â”€ LoRA&QLoRAHyperparameterConfiguration.md
â”‚   â”œâ”€â”€ loraå¾®è°ƒ.pdf
â”‚   â””â”€â”€ loraå¾®è°ƒçš„å‚æ•°æ€ä¹ˆé€‰æ‹©ï¼Œloraå’ŒQloraçš„åŒºåˆ«.md
â”‚
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ Tokenlizer_cn.md
â”‚   â””â”€â”€ Tokenlizer_en.md
â”‚
â”œâ”€â”€ UserPromptAndSystemPrompt/
â”‚   â”œâ”€â”€ What's_user_prompt_and_system_prompt_cn.md
â”‚   â””â”€â”€ What's_user_prompt_and_system_prompt_en.md
â”‚
â”œâ”€â”€ WhyLayerNormInTransformer/
â”‚   â”œâ”€â”€ Transformerä¸­ä¸ºä»€ä¹ˆä½¿ç”¨LayerNormè€Œä¸æ˜¯BatchNorm.md
â”‚   â””â”€â”€ WhyLayerNormInTransformer.md
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ MLA_in_Deepseekv2.md
â”œâ”€â”€ README_EN.md
â”œâ”€â”€ README.md
â”œâ”€â”€ ä»€ä¹ˆæ˜¯æ¿€æ´»å‡½æ•°ï¼Œå¸¸è§çš„æ¿€æ´»å‡½æ•°æœ‰å“ªäº›.md
â”œâ”€â”€ å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–.md
â””â”€â”€ é¢è¯•é—®é¢˜æ•´ç†.md
```

## File Categories

| Category | Description |
|-----------|--------------|
| **DecisionTree** | Notes on decision tree concepts, implementations, and visualizations. |
| **Essay_reading** | Academic paper reading notes (e.g., BERT). |
| **images** | Image resources such as GLU and SwiGLU diagrams. |
| **lora** | Notes on LoRA and QLoRA fine-tuning methods and hyperparameter selection. |
| **tokenizer** | Explanations of tokenizer mechanisms with both Chinese and English examples. |
| **UserPromptAndSystemPrompt** | Exploration of user and system prompt design principles. |
| **WhyLayerNormInTransformer** | Analysis of why LayerNorm is used in Transformers instead of BatchNorm. |
| **Other Individual Notes** | Covers topics like activation functions, normalization, and interview preparation. |

---

## Learning Focus
- Understanding and optimizing the **Transformer architecture**  
- **Parameter-efficient fine-tuning** techniques (PEFT / LoRA / QLoRA)  
- **Tokenizer mechanisms** and **prompt design** strategies  
- Fundamentals of **machine learning algorithms** and **numerical stability**

---

## Contribution & Usage
Contributions and suggestions are welcome!  
You may reuse or share these notes with proper attribution as stated in the **LICENSE** file.
