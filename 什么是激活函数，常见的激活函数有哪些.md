激活函数
作用：

1引入非线性，从而学习复杂的模式，如图像识别， 如果神经网络中每一层一直进行线性变换，多个堆叠起来依旧是线性模型

2 提供分段/压缩效果：
	sigmoid压缩到（0，1）可以用来表示概率问题
	Tanh压缩到（-1，1），有对称性
	ReLU可以只保留正数信息，保证了稀疏性，计算高效
3 影响梯度传播：
	激活函数决定了误差在反向传播时的梯度大小：
		sigmoid/Tanh 会导致梯度消失
		ReLU：缓解梯度消失，导致神经元死亡
4决定输出范围和语义：
	在最后一层，激活函数常常用于约束输出：
分类问题：Softmax 把输出转成概率分布。
二分类：Sigmoid 把输出变成 0–1 概率。
回归问题：有时不加激活（保持线性输出）

常见的激活函数：

1 sigmoid : 
 	f(x) = 1/1+e^(-x)
pro:
	continuous
	关于x=y对称，能够处理正负输入值
con：
	梯度消失：当输入值的绝对值较大时，输出趋近于0
	计算开销：涉及指数运算，相比ReLU等算法计算量更大

2 ReLU:
	f(x) = max(0, x)

pro：
con：
没有上限, limf → inf， 梯度爆炸
非零均值 → 归一化处理
部分神经元不会被激活 → 神经元死亡
解决方法重新定义：f(y) = y if y >= 0 and 0.001y if y <0 (Leaky ReLu)
or f(y) = ay if y<0 where a is constant (Parametric ReLU) 

3 Swish
f(x)=x⋅σ(x),其中 σ(x)=1+e−x1​
pro：结合了 Sigmoid 的平滑性和 ReLU 的稀疏性，常常比 ReLU 收敛更快、精度更高

4 GLU：

5 SwiGLU:
combination of Swish & GLU:


pro:
非线性能力：SwiGLU通过Swish激活函数引入非线性，这使得模型能够学习和表示更复杂的数据模式 。
门控特性：GLU的门控机制允许模型动态地调整信息流，使得模型在处理长序列数据时能够更好地捕捉长距离依赖关系 。
梯度稳定性：SwiGLU在负输入区域提供非零的梯度，有助于缓解梯度消失问题，从而提高模型的训练稳定性 。
可学习参数：SwiGLU的参数可以通过训练学习，使得模型可以根据不同任务和数据集动态调整，增强了模型的灵活性和适应性 。
计算效率：相比于一些复杂的激活函数，SwiGLU在保持性能的同时，具有较高的计算效率，这对于大规模语言模型的训练和推理尤为重要 。

