[中文](./DecisionTreeCn.md) |
[English](./DecisionTreeEn.md)
# 决策树 —— 过拟合、欠拟合与基尼指数

---

## 过拟合与欠拟合

| 术语 | 定义 | 原因 | 结果 |
|------|------|------|------|
| **过拟合（Overfitting）** | 模型过度拟合训练数据，学习到噪声或随机波动。 | 树太深、分裂太多、缺乏剪枝。 | 训练误差低但**测试误差高**。 |
| **欠拟合（Underfitting）** | 模型过于简单，无法捕捉数据的真实趋势。 | 树太浅、分裂太少、提前停止训练。 | 训练误差和测试误差都高。 |

**目标：** 在模型复杂度和泛化能力之间取得平衡。

---

## 决策树与过拟合

- 若不加限制，决策树容易出现过拟合。  
- 决策树通过对属性进行**二分**或**多路**划分来减少节点的不纯度。  
- 始终选择**信息量最大的划分**，例如基于基尼指数（Gini Index）或信息熵（Entropy）。

**防止过拟合的常见方法：**
1. **提前终止（Early Stopping）：**  
   在树完全分类前停止生长。  
   - 限制最大深度。  
   - 当节点样本量低于阈值时停止。  
   - 当基尼指数改进不足时停止。
2. **剪枝（Pruning）：**  
   先生成完整树，再去除不重要的分支。

---

## 基尼指数（Gini Index）

基尼指数衡量一个节点的不纯度，反映样本类别的混合程度。

### 公式

\[
\text{Gini}(t) = 1 - \sum_j [p(j|t)]^2
\]

其中 \( p(j|t) \) 为节点 \( t \) 中类别 \( j \) 的相对频率。

---

### 示例

- **完全纯净节点（单一类别）：**  
  \( \text{Gini} = 0 \)

- **最差节点（两类概率相等）：**  
  \( \text{Gini} = 1 - (1/2)^2 - (1/2)^2 = 0.5 \)

---

### 划分的基尼指数

\[
\text{Gini}_{split} = \sum_t \frac{n_t}{n} \cdot \text{Gini}(t)
\]

其中：
- \( n_t \)：子节点中的样本数  
- \( n \)：父节点样本总数  

**最优划分**是使 \( \text{Gini}_{split} \) 最小的划分（或等价地，使信息增益最大）。

---

### 课堂示例

给定：
- 父节点基尼值 = 0.49  
- 划分 1：\( \text{Gini}_{split} = 0.23 \)
- 划分 2：\( \text{Gini}_{split} = 0.47 \)

则：
\[
\text{Gain}_1 = 0.49 - 0.23 = 0.26 \quad \text{（更优划分）}
\]
\[
\text{Gain}_2 = 0.49 - 0.47 = 0.02
\]

因此选择 **划分 1**，因为它使节点更纯净。

---

## 总结

- **过拟合**：决策树过于复杂，拟合了训练集的噪声。  
- **基尼指数**：衡量节点不纯度的指标，值越低越纯。  
- 通过**剪枝**与**提前停止**可有效防止过拟合。  
- **欠拟合**：模型过于简单，无法捕捉数据规律。

---

**参考资料：** 波士顿大学 CS 506 《Decision Trees》讲义 (Lance Galletti)