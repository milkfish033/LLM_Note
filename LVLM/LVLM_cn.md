**LVLM** 是 **Large Vision-Language Model（大型视觉语言模型）** 的缩写。  
它是一类能够 **同时理解视觉信息（如图像、视频）和语言信息（如文本、语音）** 的人工智能模型。

---

## 一、定义
LVLM 是在 **大语言模型（LLM）** 的基础上扩展而来的，通过加入视觉模态的输入，使模型不仅能“读懂文字”，还能“看懂图像”。  
典型结构：

> **视觉编码器（Vision Encoder） + 大语言模型（LLM） + 融合模块（Connector / Projector）**

---

## 二、工作原理
1. **视觉编码**：用视觉模型（CLIP、ViT、ResNet、SigLIP 等）提取图像特征。  
2. **特征投射**：通过 Projector 将视觉特征映射到语言模型可理解的语义空间。  
3. **语言生成**：由大语言模型（LLaMA、Qwen、GPT 等）基于视觉语义和文本上下文生成输出。

---

## 三、常见应用
- 图像理解与描述（Image Captioning）  
- 视觉问答（VQA）  
- 图文检索（Cross-Modal Retrieval）  
- 文本引导图像分析  
- 文档/表格视觉理解  
- 多模态 RAG（图文联合检索与生成）

---

## 四、代表性模型

| 模型名称 | 机构 | 基础 LLM | 特点 |
|-----------|--------|-----------|--------|
| GPT-4V | OpenAI | GPT-4 | 支持视觉输入与多模态对话 |
| Gemini 1.5 Pro | Google DeepMind | Gemini | 支持图像、音频、视频理解 |
| LLaVA / LLaVA-Next | UC Berkeley | LLaMA | 开源代表，训练高效 |
| Qwen-VL / Qwen2-VL | 阿里巴巴 | Qwen | 中文理解能力强 |
| MiniGPT-4 / mPLUG-Owl | 华为等 | Vicuna / LLaMA | 可本地部署，轻量化 |

---

## 五、与普通 LLM 的区别

| 对比项 | LLM | LVLM |
|---------|------|------|
| 输入 | 仅文本 | 图像 + 文本 |
| 模态 | 单模态 | 多模态 |
| 能力 | 语言理解与生成 | 视觉 + 语言联合理解 |
| 应用 | 聊天、代码、文本生成 | 图文问答、视觉分析、文档解析 |

---