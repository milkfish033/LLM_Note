LayerNorm：

主要思想是对于神经网络的每一层进行批量处理，计算单个样本在当前层中的激活值的均值和方差，用这些统计量来对激活值进行归一化处理
不依赖小批量处理，减少BN中带来的噪声问题， 同时允许使用更小的小批量大小进行训练

Transformer中为什么使用LayerNorm而不是BatchNorm：
1解决BN带来的噪声问题
	BN中的噪声问题：
		原因：训练时使用mini-batch估计平均值和方差，本身有误差
			训练时使用mini-batch而推理时使用全局moving average统计量

2 确保参与归一化的数据本质上是可以比较的
3 
BN关注的是同一批次中特定的一列的数据，是不同样本的同一特征，因为这些特征在统计上是相关的，所以他们可以合理地被放在一起进行归一化处理

LN适用于不同样本之间不能直接进行比较的情况， 对于transformer的自注意力机制， 每个位置上的数据代表了不同的特征，用LN对每个样本的所有特征做归一化处理