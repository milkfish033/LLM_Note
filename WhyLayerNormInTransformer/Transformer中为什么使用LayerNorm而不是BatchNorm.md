
[中文](./Transformer中为什么使用LayerNorm而不是BatchNorm.md) |
[English](./WhyLayerNormInTransformer.md)

# LayerNorm

## 基本思想
- 对神经网络的每一层进行批量处理  
- 计算单个样本在当前层中的激活值的均值和方差  
- 用这些统计量来对激活值进行归一化处理  

特点：  
- 不依赖小批量处理  
- 减少 BN 中带来的噪声问题  
- 允许使用更小的小批量大小进行训练  

---

## Transformer 中为什么使用 LayerNorm 而不是 BatchNorm

1. **解决 BN 带来的噪声问题**  
   - BN 的噪声问题：  
     - 训练时使用 mini-batch 估计平均值和方差，本身有误差  
     - 训练时使用 mini-batch，而推理时使用全局 moving average 统计量  

2. **确保参与归一化的数据本质上是可以比较的**  

3. **BN 的归一化方式**  
   - BN 关注的是同一批次中特定的一列数据（不同样本的同一特征）  
   - 因为这些特征在统计上相关，所以可以合理地放在一起进行归一化处理  

4. **LN 的适用场景**  
   - LN 适用于不同样本之间不能直接进行比较的情况  
   - 对于 Transformer 的自注意力机制：  
     - 每个位置上的数据代表了不同的特征  
     - 使用 LN 对每个样本的所有特征做归一化处理  
